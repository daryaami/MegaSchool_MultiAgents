# MegaSchool Multi-Agent Interview Coach

## Оглавление

1. [Обзор проекта](#обзор-проекта)
2. [Ключевые особенности](#ключевые-особенности)
3. [Архитектура мультиагентной системы](#архитектура-мультиагентной-системы)
4. [RAG (Retrieval-Augmented Generation)](#rag-retrieval-augmented-generation)
5. [Используемые модели](#используемые-модели)
6. [Технологический стек](#технологический-стек)
7. [Структура проекта](#структура-проекта)
8. [Разворачивание проекта](#разворачивание-проекта)
9. [Заключение](#заключение)

---

## Обзор проекта

**Multi-Agent Interview Coach** — это система для проведения технических интервью с использованием мультиагентной архитектуры и больших языковых моделей. Система автоматизирует процесс собеседования, адаптивно подстраивая сложность вопросов под уровень кандидата и предоставляя детальный анализ его ответов.

---

## Ключевые особенности

### 1. **Мультиагентная архитектура**
- Три специализированных агента, работающих асинхронно
- Чёткое разделение ответственности между агентами
- Асинхронная коммуникация через очереди сообщений

### 2. **Адаптивная сложность вопросов**
- Автоматическая адаптация сложности на основе оценки ответов
- Три уровня действий: `increase`, `same`, `decrease`
- Умное переключение между темами

### 3. **Детекция галлюцинаций и off-topic**
- Автоматическое обнаружение выдуманных фактов
- Определение нерелевантных ответов
- Обработка role-reversal (когда кандидат задаёт вопросы)

### 4. **RAG-интеграция**
- Поиск релевантных материалов из базы знаний
- Улучшение качества оценки через контекст
- Использование FAISS для быстрого семантического поиска

### 5. **Структурированный вывод**
- Pydantic схемы для валидации ответов LLM
- Типобезопасность и гарантированная структура данных
- Автоматическая валидация всех ответов агентов

### 6. **Веб-интерфейс и CLI**
- Интерактивный веб-интерфейс на Flask
- CLI режим для автоматизации
- Поддержка Docker для развёртывания

---

## Архитектура мультиагентной системы

### Общая концепция

Система построена на принципах **мультиагентной архитектуры**, где каждый агент выполняет свою специализированную роль и взаимодействует с другими через асинхронные очереди сообщений.

### Компоненты системы

#### 1. **Orchestrator** (`src/orchestrator.py`)
**Роль:** Координатор всей системы

**Функции:**
- Инициализирует все компоненты системы
- Создаёт очереди для коммуникации между агентами
- Запускает асинхронные циклы агентов
- Управляет жизненным циклом интервью
- Сохраняет финальные логи в `logs/interview_log_{session_id}.json`

**Ключевые моменты:**
- Использует `asyncio` для асинхронного выполнения
- Управляет сессиями через `SessionLogger`
- Инициализирует RAG при необходимости

#### 2. **Interviewer** (`src/agents/interviewer.py`)
**Роль:** Публичный агент, взаимодействующий с кандидатом

**Функции:**
- Генерирует вопросы для кандидата
- Принимает ответы пользователя
- Получает рекомендации от Observer
- Адаптирует сложность вопросов
- Обрабатывает role-reversal (отвечает на вопросы кандидата)
- Логирует все ходы интервью

**Ключевые особенности:**
- Использует LLM для генерации персонализированных вопросов
- Поддерживает внутренние рассуждения (reasoning)
- Добавляет комментарии (одобрения/подсказки) к вопросам
- Избегает повторения уже заданных вопросов
- Управляет переключением между темами

**Взаимодействие:**
- Получает сообщения из `interviewer_in` очереди
- Отправляет вопросы в `user_out` очередь
- Отправляет запросы на анализ в `observer_queue`
- Получает результаты анализа от Observer

#### 3. **Observer** (`src/agents/observer.py`)
**Роль:** Внутренний аналитик, оценивающий ответы кандидата

**Функции:**
- Анализирует ответы кандидата на правильность
- Оценивает уверенность кандидата
- Определяет галлюцинации и off-topic ответы
- Предлагает темы для следующих вопросов
- Определяет role-reversal и stop_intent
- Использует RAG для улучшения оценки

**Ключевые особенности:**
- Использует LLM для глубокого анализа ответов
- Интегрирован с RAG для поиска релевантных материалов
- Генерирует структурированные оценки (correctness, confidence)
- Определяет действия для адаптации сложности
- Имеет fallback на эвристики Policy при недоступности LLM
- Поддерживает cooldown при ошибках LLM

**Взаимодействие:**
- Получает запросы на анализ из `observer_queue`
- Использует RAG для поиска релевантных материалов
- Отправляет результаты анализа обратно через `reply_queue`

#### 4. **Manager** (`src/agents/manager.py`)
**Роль:** Финальный аналитик, генерирующий отчёт

**Функции:**
- Анализирует всю историю интервью
- Генерирует финальный отчёт с вердиктом
- Оценивает технические навыки и soft skills
- Создаёт персональный roadmap для кандидата
- Принимает решение о найме (Hire/No Hire/Strong Hire)

**Ключевые особенности:**
- Активируется только в конце интервью
- Использует LLM для генерации структурированного отчёта
- Анализирует статистику по всем темам
- Оценивает соответствие заявленному грейду

**Взаимодействие:**
- Получает команду `finalize` из `manager_in` очереди
- Использует данные из `SessionLogger`
- Возвращает финальный отчёт через `reply_queue`

#### 5. **SessionLogger** (`src/session.py`)
**Роль:** Хранилище состояния интервью

**Функции:**
- Хранит историю всех вопросов и ответов
- Сохраняет внутренние мысли агентов
- Ведёт наблюдения Observer
- Формирует финальный отчёт
- Сериализует данные в JSON

**Структура данных:**
- `turns`: список всех ходов интервью
- `observations`: наблюдения Observer по темам
- `history`: история вопросов и ответов
- `final_feedback`: финальный отчёт Manager

### Поток данных в системе

#### Один ход (turn) интервью:

```
1. Interviewer генерирует вопрос
   ↓
2. Вопрос отправляется кандидату (через user_out очередь)
   ↓
3. Кандидат отвечает
   ↓
4. Ответ отправляется в Observer (через observer_queue)
   ↓
5. Observer анализирует ответ:
   - Использует RAG для поиска релевантных материалов
   - Отправляет запрос в LLM с опорными материалами
   - Получает структурированный анализ
   - Определяет action (increase/same/decrease)
   - Генерирует internal_thoughts
   ↓
6. Результат анализа возвращается в Interviewer
   ↓
7. Interviewer:
   - Логирует ход (включая internal_thoughts)
   - Принимает решение о следующем вопросе
   - Генерирует новый вопрос с учётом рекомендаций
   - Отправляет вопрос кандидату
   ↓
8. SessionLogger сохраняет ход в историю
```

#### Завершение интервью:

```
1. Кандидат отправляет stop_intent или завершает интервью
   ↓
2. Orchestrator отправляет команду finalize в Manager
   ↓
3. Manager:
   - Анализирует всю историю интервью
   - Генерирует финальный отчёт через LLM
   - Возвращает структурированный отчёт
   ↓
4. SessionLogger сохраняет финальный отчёт
   ↓
5. Все данные сериализуются в JSON файл
```

### Асинхронная коммуникация

Все агенты работают асинхронно через `asyncio.Queue`:

- **interviewer_in**: очередь для команд Interviewer
- **observer_in**: очередь для запросов Observer
- **manager_in**: очередь для команд Manager
- **user_out**: очередь для сообщений пользователю
- **reply_queue**: временные очереди для ответов между агентами

### Обработка ошибок и fallback

Система имеет несколько уровней отказоустойчивости:

1. **LLM недоступен:**
   - Observer использует эвристики Policy
   - Interviewer использует базовые вопросы из конфигурации
   - Manager использует упрощённый отчёт из SessionLogger

2. **Timeout обработки:**
   - Настраиваемые таймауты для каждого агента
   - Graceful degradation при превышении таймаута

3. **RAG недоступен:**
   - Система продолжает работу без RAG
   - Observer использует только LLM для анализа

---

## RAG (Retrieval-Augmented Generation)

### Что такое RAG в контексте проекта

**RAG (Retrieval-Augmented Generation)** — это техника, которая улучшает качество анализа ответов кандидата путём поиска релевантных материалов из базы знаний и использования их как контекста для LLM.

### Архитектура RAG в проекте

#### Компоненты RAG

1. **RAGRetriever** (`src/rag.py`)
   - Класс для поиска релевантных материалов
   - Использует sentence-transformers для создания эмбеддингов
   - Использует FAISS для быстрого поиска по эмбеддингам

2. **База данных**
   - `data/rag/data.pkl`: pandas DataFrame с вопросами и ответами
   - `data/rag/faiss_index.bin`: FAISS индекс для поиска

3. **Интеграция с Observer**
   - RAG вызывается при анализе каждого ответа
   - Найденные материалы добавляются в промпт для LLM

### Как работает RAG

#### 1. Инициализация

```python
rag = RAGRetriever(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    index_path="data/rag/faiss_index.bin",
    data_path="data/rag/data.pkl"
)
```

- Загружается модель для создания эмбеддингов
- Загружается FAISS индекс
- Загружаются данные из pickle файла

#### 2. Поиск релевантных материалов

Когда Observer анализирует ответ кандидата:

```python
query = f"{question} {answer}"  # Комбинация вопроса и ответа
results = rag.search(query, top_k=5, min_relevance=0.65)
```

**Процесс поиска:**
1. Текст запроса кодируется в эмбеддинг через sentence-transformers
2. Эмбеддинг нормализуется (L2 normalization)
3. FAISS ищет наиболее похожие эмбеддинги в индексе
4. Результаты фильтруются по минимальной релевантности
5. Возвращаются top_k наиболее релевантных записей

#### 3. Форматирование материалов

Найденные материалы форматируются в текстовый вид:

```
ОПОРНЫЕ МАТЕРИАЛЫ ИЗ БАЗЫ ДАННЫХ ИНТЕРВЬЮ:

1. Категория: Python | Навык: Data Structures | Уровень: Junior
   Вопрос: В чем разница между списком и кортежем в Python?
   Ожидаемый ответ: Список (list) - изменяемая структура...
   Релевантность: 0.85

2. Категория: Python | Навык: OOP | Уровень: Middle
   Вопрос: Что такое наследование в Python?
   Ожидаемый ответ: Наследование позволяет создавать новые классы...
   Релевантность: 0.72
```

#### 4. Интеграция в промпт

Опорные материалы добавляются в начало промпта для LLM:

```
ОПОРНЫЕ МАТЕРИАЛЫ ИЗ БАЗЫ ДАННЫХ ИНТЕРВЬЮ:
[форматированные материалы]

Проанализируй ответ кандидата...
[основные инструкции]

Вопрос: {question}
Ответ: {answer}
```

### Преимущества RAG в проекте

1. **Улучшение качества оценки**
   - LLM получает контекст о правильных ответах
   - Более точная оценка correctness и confidence

2. **Обнаружение галлюцинаций**
   - Сравнение ответа кандидата с ожидаемыми ответами
   - Выявление выдуманных фактов

3. **Предложение правильных ответов**
   - При обнаружении пробелов в знаниях
   - LLM может предложить правильный ответ на основе материалов

4. **Адаптация к разным темам**
   - База знаний может содержать материалы по разным технологиям
   - Система автоматически находит релевантные материалы

### Структура данных RAG

База данных содержит pandas DataFrame со следующими колонками:

- **Category**: категория вопроса (например, "Python", "SQL", "System Design")
- **Skill**: конкретный навык (например, "Data Structures", "ORM", "Transactions")
- **Level**: уровень сложности (Junior, Middle, Senior)
- **Question**: текст вопроса
- **Answer**: ожидаемый правильный ответ

### Настройка RAG

Параметры настраиваются в `config/runtime.json`:

```json
{
  "observer": {
    "rag": {
      "enabled": true,
      "top_k": 5,
      "min_relevance": 0.65,
      "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "index_path": null,
      "data_path": null,
      "base_dir": null
    }
  }
}
```

- **enabled**: включить/выключить RAG
- **top_k**: количество результатов для поиска
- **min_relevance**: минимальный порог релевантности (0.0-1.0)
- **model_name**: модель для создания эмбеддингов

### Важные моменты использования RAG

1. **RAG не обязателен** — система работает и без него
2. **Материалы как справочная информация** — LLM использует их для контекста, но не как строгие правила
3. **Поиск по комбинации** — запрос формируется из вопроса и ответа для лучшей релевантности
4. **Фильтрация по релевантности** — только материалы с достаточной релевантностью используются

---

## Используемые модели

### LLM модели (Большие языковые модели)

Система поддерживает два провайдера LLM, которые используются всеми агентами для генерации вопросов, анализа ответов и создания отчётов.

#### 1. Google Gemini

**Модель по умолчанию:** `gemini-3-flash-preview`

**Использование:**
- **Interviewer**: генерация персонализированных вопросов для кандидата
- **Observer**: глубокий анализ ответов кандидата, определение галлюцинаций, оценка правильности
- **Manager**: генерация финального структурированного отчёта с вердиктом

**Настройка:**
- Переменная окружения: `GEMINI_API_KEY`
- Модель задаётся через: `GEMINI_MODEL` (по умолчанию `gemini-3-flash-preview`)
- Провайдер: `LLM_PROVIDER=gemini`

**Особенности:**
- Быстрая генерация ответов
- Поддержка структурированного вывода через Pydantic схемы
- Температура по умолчанию: 0.2 (для более детерминированных ответов)

#### 2. Mistral AI

**Модель по умолчанию:** `mistral-large-latest`

**Использование:**
- Те же функции, что и Gemini (Interviewer, Observer, Manager)
- Альтернативный провайдер для тех же задач

**Настройка:**
- Переменная окружения: `MISTRAL_API_KEY`
- Модель задаётся через: `MISTRAL_MODEL` (по умолчанию `mistral-large-latest`)
- Base URL: `MISTRAL_BASE_URL` (по умолчанию `https://api.mistral.ai/v1`)
- Провайдер: `LLM_PROVIDER=mistral`

**Особенности:**
- Поддержка официального SDK и REST API
- Высокое качество генерации для сложных задач анализа
- Температура по умолчанию: 0.2

**Переключение между провайдерами:**
Система автоматически определяет доступный провайдер на основе переменных окружения. Если указаны оба ключа, приоритет отдаётся Mistral, если не указан `LLM_PROVIDER`.

### Модель для эмбеддингов (RAG)

**Модель:** `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

**Назначение:**
- Создание векторных представлений (эмбеддингов) текста для семантического поиска
- Используется в RAG системе для поиска релевантных материалов из базы знаний

**Характеристики:**
- **Тип**: Sentence Transformer
- **Языки**: Мультиязычная поддержка (включая русский и английский)
- **Размерность эмбеддинга**: 384
- **Архитектура**: MiniLM (упрощённая версия BERT)
- **Размер модели**: ~471 MB (скачивается при первом запуске)

**Использование:**
1. **Инициализация**: Модель загружается при создании `RAGRetriever`
2. **Кодирование запросов**: Текст вопроса и ответа кандидата преобразуется в эмбеддинг
3. **Поиск**: FAISS использует эмбеддинги для поиска похожих записей в базе знаний

**Настройка:**
Модель настраивается в `config/runtime.json`:
```json
{
  "observer": {
    "rag": {
      "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    }
  }
}
```

**Альтернативные модели:**
Можно использовать другие модели sentence-transformers, например:
- `sentence-transformers/all-MiniLM-L6-v2` (английский, быстрее)
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (выше качество, больше размер)

### Интеграция моделей в систему

#### Поток использования LLM:

```
1. Агент формирует промпт
   ↓
2. LLMClient отправляет запрос в выбранный провайдер (Gemini/Mistral)
   ↓
3. LLM генерирует ответ
   ↓
4. Ответ валидируется через Pydantic схемы
   ↓
5. Структурированные данные возвращаются агенту
```

#### Поток использования эмбеддингов:

```
1. Observer получает вопрос и ответ кандидата
   ↓
2. RAGRetriever формирует запрос: "{question} {answer}"
   ↓
3. SentenceTransformer кодирует запрос в эмбеддинг
   ↓
4. FAISS ищет похожие эмбеддинги в индексе
   ↓
5. Найденные материалы форматируются и добавляются в промпт для LLM
   ↓
6. LLM анализирует ответ с контекстом из RAG
```

### Требования к ресурсам

**LLM модели:**
- Работают через API, не требуют локальных ресурсов
- Необходимы API ключи от провайдеров
- Зависит от тарифов провайдера (количество токенов, запросов)

**Модель эмбеддингов:**
- Требует ~500 MB оперативной памяти
- Первая загрузка: ~471 MB дискового пространства (кэш)
- Последующие загрузки: из кэша (быстро)
- Рекомендуется: минимум 2 GB свободной RAM для комфортной работы

### Fallback механизмы

При недоступности моделей система использует fallback:

1. **LLM недоступен:**
   - Observer → эвристики Policy для оценки
   - Interviewer → базовые вопросы из конфигурации
   - Manager → упрощённый отчёт из SessionLogger

2. **RAG недоступен:**
   - Система продолжает работу без RAG
   - Observer использует только LLM для анализа (без опорных материалов)

---

## Технологический стек

### Backend
- **Python 3.10+** — основной язык программирования
- **asyncio** — асинхронное программирование
- **Flask** — веб-фреймворк для UI
- **Pydantic** — валидация данных и структурированный вывод

### LLM интеграция
- **Google Gemini API** — поддержка Gemini моделей
- **Mistral AI API** — поддержка Mistral моделей
- Абстракция `LLMClient` для переключения между провайдерами

### RAG
- **sentence-transformers** — создание эмбеддингов
- **FAISS** — быстрый поиск по эмбеддингам
- **pandas** — работа с данными

### Инфраструктура
- **Docker** — контейнеризация
- **Docker Compose** — оркестрация контейнеров

---

## Структура проекта

```
megaschool/
├── config/
│   └── runtime.json          # Конфигурация системы
├── data/
│   └── rag/                   # RAG данные
│       ├── faiss_index.bin    # FAISS индекс
│       ├── data.pkl           # Данные в формате pickle
│       └── README.md
├── src/
│   ├── agents/               # Агенты системы
│   │   ├── base.py           # Базовый класс Agent
│   │   ├── interviewer.py    # Interviewer агент
│   │   ├── observer.py       # Observer агент
│   │   └── manager.py        # Manager агент
│   ├── orchestrator.py       # Координатор системы
│   ├── session.py            # Управление сессиями
│   ├── rag.py                # RAG retriever
│   ├── llm.py                # LLM клиенты
│   ├── schemas.py            # Pydantic схемы
│   ├── policy.py             # Эвристики и fallback
│   ├── score.py              # Оценка ответов
│   └── web_ui.py             # Веб-интерфейс
├── templates/                 # HTML шаблоны
├── static/                   # CSS и JS
├── logs/                     # Логи интервью
├── tests/                    # Тесты
└── requirements.txt          # Зависимости
```

---

## Разворачивание проекта

### Требования

- **Python 3.10+** (рекомендуется 3.12)
- **Docker и Docker Compose** (опционально, для контейнеризации)
- **API ключ** от одного из провайдеров LLM:
  - Google Gemini API ключ ([получить здесь](https://makersuite.google.com/app/apikey))
  - Или Mistral AI API ключ ([получить здесь](https://console.mistral.ai/))

### Шаг 1: Получение проекта

Клонируйте репозиторий или скачайте проект:

```bash
git clone https://github.com/daryaami/MegaSchool_MultiAgents.git
```

### Шаг 2: Установка зависимостей

#### Вариант A: Локальная установка (без Docker)

1. **Создайте виртуальное окружение:**

   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate

   # Linux/Mac
   python3 -m venv venv
   source venv/bin/activate
   ```

2. **Установите зависимости:**

   ```bash
   pip install -r requirements.txt
   ```

#### Вариант B: Использование Docker

Убедитесь, что Docker и Docker Compose установлены:

```bash
docker --version
docker-compose --version
```

### Шаг 3: Настройка переменных окружения

Создайте файл `.env` в корне проекта:

**Для Gemini:**
```env
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-3-flash-preview
LLM_PROVIDER=gemini
```

**Для Mistral:**
```env
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_MODEL=mistral-large-latest
MISTRAL_BASE_URL=https://api.mistral.ai/v1
LLM_PROVIDER=mistral
```

> **Важно:** Замените `your_gemini_api_key_here` или `your_mistral_api_key_here` на ваш реальный API ключ.

### Шаг 4: Настройка RAG (опционально)

Если вы хотите использовать RAG для улучшения качества оценки:

1. Поместите файлы в директорию `data/rag/`:
   - `faiss_index.bin` — FAISS индекс для поиска
   - `data.pkl` — данные с вопросами и ответами в формате pickle

2. Убедитесь, что RAG включен в `config/runtime.json`:
   ```json
   {
     "observer": {
       "rag": {
         "enabled": true,
         "top_k": 5,
         "min_relevance": 0.65
       }
     }
   }
   ```

> **Примечание:** Система работает и без RAG. Если файлы отсутствуют, RAG будет автоматически отключён.

### Шаг 5: Проверка настройки

Проверьте, что LLM доступен:

```bash
# Для Gemini
python -m src.llm_check --provider gemini --prompt "ping"

# Для Mistral
python -m src.llm_check --provider mistral --prompt "ping"
```

Если всё настроено правильно, вы увидите ответ от модели.

### Шаг 6: Запуск проекта

#### Вариант A: CLI режим (Командная строка)

1. **Создайте файл `input.json` с данными кандидата:**

```json
{
  "team_name": "Team Alpha",
  "candidate": {
    "name": "Алекс",
    "position": "Backend Developer",
    "grade": "Junior",
    "experience": "Пет-проекты на Django, немного SQL"
  }
}
```

2. **Запустите интервью:**

```bash
python -m src.orchestrator --input input.json --config config/runtime.json
```

3. **Использование:**
   - Отвечайте на вопросы интервьюера в консоли
   - Для завершения интервью введите `stop_interview` или `стоп`
   - После завершения будет выведен финальный отчёт в консоль
   - Лог интервью сохранится в `logs/interview_log_{session_id}.json`

**Параметры командной строки:**
- `--input` — путь к JSON файлу с данными кандидата (по умолчанию: `input.json`)
- `--config` — путь к конфигурационному файлу (по умолчанию: `config/runtime.json`)

#### Вариант B: Веб-интерфейс (WebUI)

**Локальный запуск:**

1. **Запустите веб-сервер:**

```bash
python run_web.py
```

2. **Откройте в браузере:**

```
http://localhost:5000
```

3. **Использование веб-интерфейса:**
   - Заполните форму с данными кандидата (имя, позиция, грейд, опыт)
   - Нажмите "Начать интервью"
   - Отвечайте на вопросы в интерактивном чате
   - Внутренние мысли агентов отображаются серым цветом
   - После завершения интервью будет показан финальный отчёт
   - Лог интервью автоматически сохранится в `logs/`

**Запуск с Docker:**

1. **Соберите и запустите контейнер:**

```bash
docker-compose up --build
```

2. **Или в фоновом режиме:**

```bash
docker-compose up -d --build
```

3. **Откройте в браузере:**

```
http://localhost:5000
```

4. **Остановка:**

```bash
docker-compose down
```

### Шаг 7: Проверка работоспособности

#### CLI режим:

1. Запустите интервью через CLI
2. Ответьте на несколько вопросов
3. Проверьте, что финальный отчёт выводится в консоль
4. Проверьте папку `logs/` — там должен быть создан файл `interview_log_{session_id}.json`

#### Веб-интерфейс:

1. Откройте http://localhost:5000
2. Заполните форму с данными кандидата
3. Начните интервью
4. Проверьте, что вопросы генерируются и ответы обрабатываются
5. Проверьте, что внутренние мысли агентов отображаются
6. Завершите интервью и проверьте финальный отчёт
7. Проверьте папку `logs/` — там должен быть создан файл лога

### Структура проекта после разворачивания

```
megaschool/
├── .env                    # Ваши API ключи (не коммитится в git)
├── config/
│   └── runtime.json       # Конфигурация системы
├── data/
│   └── rag/               # RAG данные (опционально)
│       ├── faiss_index.bin
│       └── data.pkl
├── logs/                  # Логи интервью (создаются автоматически)
├── flask_session/         # Сессии Flask (создаются автоматически)
├── src/                   # Исходный код
├── templates/             # HTML шаблоны
├── static/                # CSS и JS
├── requirements.txt       # Python зависимости
├── Dockerfile             # Конфигурация Docker
└── docker-compose.yml     # Конфигурация Docker Compose
```

### Быстрый старт

**Минимальная настройка для CLI:**

```bash
# 1. Создайте виртуальное окружение
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# 2. Установите зависимости
pip install -r requirements.txt

# 3. Создайте .env файл с API ключом
echo "GEMINI_API_KEY=your_key" > .env
echo "GEMINI_MODEL=gemini-3-flash-preview" >> .env
echo "LLM_PROVIDER=gemini" >> .env

# 4. Создайте input.json
cat > input.json << EOF
{
  "team_name": "Team Alpha",
  "candidate": {
    "name": "Алекс",
    "position": "Backend Developer",
    "grade": "Junior",
    "experience": "Пет-проекты на Django, немного SQL"
  }
}
EOF

# 5. Запустите интервью
python -m src.orchestrator --input input.json --config config/runtime.json
```

**Минимальная настройка для WebUI:**

```bash
# 1-3. Те же шаги, что и для CLI

# 4. Запустите веб-сервер
python run_web.py

# 5. Откройте http://localhost:5000 в браузере
```

---

## Заключение

Multi-Agent Interview Coach представляет собой современную систему для автоматизации технических интервью, объединяющую:

- **Мультиагентную архитектуру** для гибкого и масштабируемого решения
- **RAG** для улучшения качества оценки
- **Структурированный вывод** для надёжности и типобезопасности
- **Адаптивность** для персонализации процесса интервью
