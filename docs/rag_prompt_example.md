# Пример работы RAG в Observer

## Как это работает

1. **Observer получает вопрос и ответ кандидата**
2. **RAG ищет релевантные материалы** из базы данных по комбинации вопроса и ответа
3. **Опорные материалы форматируются** в текстовый вид
4. **Материалы добавляются в начало промпта** перед основными инструкциями
5. **LLM анализирует ответ** с учетом опорных материалов

## Формат опорных материалов

Опорные материалы форматируются следующим образом:

```
ОПОРНЫЕ МАТЕРИАЛЫ ИЗ БАЗЫ ДАННЫХ ИНТЕРВЬЮ:
(Используй эти материалы как справочную информацию, не как строгие правила)

1. Категория: Machine Learning | Навык: Feature Engineering | Уровень: Intermediate
   Вопрос: Какие техники feature engineering вы применяли в своих проектах?
   Ожидаемый ответ: Feature engineering включает создание новых признаков, нормализацию, кодирование категориальных переменных...
   Релевантность: 0.85

2. Категория: Machine Learning | Навык: Model Selection | Уровень: Intermediate
   Вопрос: Как вы выбираете модель для задачи?
   Ожидаемый ответ: Выбор модели зависит от типа задачи, размера данных, требований к интерпретируемости...
   Релевантность: 0.72
...
```

## Полный пример промпта

### System Prompt (неизменный):
```
Ты внутренний наблюдатель. Анализируй ответы кандидата на технические вопросы. 
Оцени ответ (correctness, confidence), предложи тему для следующего вопроса 
(suggested_topic), определи role_reversal (когда кандидат задаёт вопрос интервьюеру) 
и stop_intent (когда хочет завершить интервью). Верни только JSON без Markdown.
```

### User Prompt (с RAG материалами):

```
ОПОРНЫЕ МАТЕРИАЛЫ ИЗ БАЗЫ ДАННЫХ ИНТЕРВЬЮ:
(Используй эти материалы как справочную информацию, не как строгие правила)

1. Категория: Machine Learning | Навык: Feature Engineering | Уровень: Intermediate
   Вопрос: Какие техники feature engineering вы применяли в своих проектах?
   Ожидаемый ответ: Feature engineering включает создание новых признаков из существующих, нормализацию числовых признаков (StandardScaler, MinMaxScaler), кодирование категориальных переменных (One-Hot Encoding, Label Encoding), обработку пропусков (заполнение средним, медианой, предсказание), создание полиномиальных признаков, извлечение признаков из дат (день недели, месяц), работа с текстовыми данными (TF-IDF, word embeddings).
   Релевантность: 0.85

2. Категория: Machine Learning | Навык: Model Selection | Уровень: Intermediate
   Вопрос: Как вы выбираете модель для задачи машинного обучения?
   Ожидаемый ответ: Выбор модели зависит от типа задачи (классификация, регрессия), размера данных, требований к интерпретируемости, наличия структурированных или неструктурированных данных. Для небольших данных - логистическая регрессия или SVM, для больших - Random Forest, XGBoost, для изображений - CNN, для текста - RNN/LSTM или трансформеры.
   Релевантность: 0.72

3. Категория: Python | Навык: Data Structures | Уровень: Junior
   Вопрос: В чем разница между списком и кортежем в Python?
   Ожидаемый ответ: Список (list) - изменяемая (mutable) структура данных, можно добавлять, удалять, изменять элементы. Кортеж (tuple) - неизменяемая (immutable) структура, после создания нельзя изменить. Списки используют квадратные скобки [], кортежи - круглые (). Кортежи быстрее и занимают меньше памяти, используются как ключи словарей.
   Релевантность: 0.68

Проанализируй ответ кандидата. Верни СТРОГО JSON без пояснений и markdown.

ПРАВИЛА:

1. stop_intent:
   - true: явное желание завершить ("стоп", "закончим", "давай фидбэк", "хватит")
   - false: признание незнания ("не знаю", "не умею") или любой технический ответ

2. role_reversal:
   - true: кандидат задаёт вопрос интервьюеру (содержит "?" и обращение: "у вас", "ваши", "вам", "можете", "расскажите")
     Примеры: "Что такое X?", "Какие у вас премии?", "Слушайте, а как...?", "Можете уточнить?"
   - false: просьба сменить тему ("спросите что-то другое") или признание незнания ("Не знаю что такое X")

3. suggested_topic:
   - Предложи тему на основе пробелов или необходимости углубиться
   - Примеры: 'SQL Transactions', 'Django ORM', 'Python Data Structures', 'Error Handling'
   - Пустая строка, если всё хорошо
   - ВАЖНО: Не углубляйся слишком долго в одну тему. После 1-2 вопросов по одной теме лучше переключиться на другую или вернуться к общей теме (пустая строка)

4. ОЦЕНКИ (correctness и confidence):
   - correctness (0.0-1.0): правильность и полнота ответа
     * 0.9-1.0: отличный ответ - полный, правильный, релевантный, с примерами/деталями
     * 0.7-0.89: хороший ответ - правильный, но может не хватать деталей или примеров
     * 0.5-0.69: средний ответ - частично правильный, есть неточности или пробелы
     * 0.3-0.49: слабый ответ - много неточностей или значительные пробелы
     * 0.0-0.29: плохой ответ - неправильный или нерелевантный
   - confidence (0.0-1.0): уверенность кандидата в ответе
     * 0.8-1.0: высокая уверенность - ответ четкий, без колебаний
     * 0.6-0.79: средняя уверенность - ответ уверенный, но могут быть сомнения
     * 0.4-0.59: низкая уверенность - ответ неуверенный, много "возможно", "наверное"
     * 0.0-0.39: очень низкая уверенность - явные сомнения или признание незнания
   - ВАЖНО: Если ответ полный, релевантный, упомянуты основные моменты и есть примеры - ставь correctness 0.9-1.0, а не 0.8!

ФОРМАТ JSON:
{"action":"increase|same|decrease","scores":{"correctness":0.0-1.0,"confidence":0.0-1.0},"notes":"кратко","status":"confirmed|gap","correct_answer":"если есть пробел","hallucination":true|false,"hallucination_reason":"кратко","off_topic":true|false,"off_topic_reason":"кратко","stop_intent":true|false,"stop_intent_reason":"кратко","role_reversal":true|false,"role_reversal_reason":"кратко","suggested_topic":"тема или пустая строка"}

Вопрос: Какие техники feature engineering вы применяли в своих проектах для улучшения качества моделей?
Ответ: Я использовал нормализацию данных через StandardScaler, создавал новые признаки из существующих, например, из даты извлекал день недели и месяц. Также применял One-Hot Encoding для категориальных переменных и обрабатывал пропуски, заполняя их медианой.
```

## Как данные передаются в LLM

1. **System Prompt** - отправляется один раз, определяет роль агента
2. **User Prompt** - отправляется каждый раз, содержит:
   - Опорные материалы из RAG (если найдены релевантные)
   - Основные инструкции по анализу
   - Конкретный вопрос и ответ кандидата

## Параметры поиска

Настраиваются в `config/runtime.json`:
- `top_k: 5` - количество результатов для поиска
- `min_relevance: 0.6` - минимальный порог релевантности (0.0-1.0)

## Важные моменты

1. **Опорные материалы не обязательны** - если ничего не найдено или релевантность низкая, промпт отправляется без них
2. **Материалы как справочная информация** - LLM использует их для контекста, но не как строгие правила
3. **Поиск по комбинации** - запрос формируется как `f"{question} {answer}"`, что позволяет найти материалы, релевантные и вопросу, и ответу
