# Тесты сценариев интервью

Интеграционные тесты проверяют, как агенты системы реагируют на разные сценарии интервью.

## Быстрый старт

```bash
# Активируйте виртуальное окружение
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Запустите все тесты
pytest tests/test_interview_scenarios.py -v

# Конкретный тест
pytest tests/test_interview_scenarios.py::test_ideal_candidate_scenario -v
```

## Сценарии

### Идеальный кандидат
- Высокие оценки (correctness > 0.7), детальные ответы
- Ожидается: "Strong Hire" или "Hire", минимум пробелов

### Средний кандидат
- Смешанные оценки (0.3-0.7), честное признание незнания
- Ожидается: "Hire" или "No Hire", есть пробелы, но честность

### Плохой кандидат
- Низкие оценки (< 0.5), галлюцинации, неверные ответы
- Ожидается: "No Hire", много пробелов, обнаружены галлюцинации

## Как это работает

Тесты используют **MockLLM** - мок-объект, который заменяет реальный LLM и возвращает предопределенные ответы:

1. **Observer** → анализирует ответ кандидата → возвращает `action`, `scores`, `status`
2. **Interviewer** → генерирует вопрос на основе рекомендаций Observer
3. **Manager** → создает финальный отчёт с вердиктом

MockLLM определяет тип запроса по содержимому промпта и возвращает соответствующий ответ в зависимости от сценария.

**Что тестируем:**
- ✅ Логику агентов и их взаимодействие
- ✅ Реакции на разные типы кандидатов
- ✅ Структуру данных и формат ответов

**Что НЕ тестируем:**
- ❌ Качество вопросов реального LLM
- ❌ Точность анализа реального LLM

## Что проверяют тесты

- **Оценки** (correctness, confidence) соответствуют сценарию
- **Действия** (increase/same/decrease) корректны
- **Финальный вердикт** (Hire/No Hire/Strong Hire) правильный
- **Обнаружение** галлюцинаций и пробелов в знаниях
- **Структура** финального отчёта корректна

## Расширение

Добавить новый сценарий:

1. Обновите `MockLLM._get_observer_response()`:
```python
elif self.scenario == "new_scenario":
    return json.dumps({"action": "same", "scores": {...}, ...})
```

2. Создайте тест:
```python
@pytest.mark.asyncio
async def test_new_scenario():
    responses = ["Ответ 1", "Ответ 2"]
    result = await run_interview_scenario("new_scenario", responses)
    # Проверки
```

## Отладка

Если тесты не проходят:
- Убедитесь, что активировано виртуальное окружение
- Проверьте, что `config/runtime.json` существует
- Установите зависимости: `pip install -r requirements.txt`
- Запустите с `-s` для подробного вывода: `pytest -v -s`

## Примеры

JSON-примеры интервью находятся в `examples/interview_scenarios/`:
- `ideal_candidate.json`
- `average_candidate.json`
- `poor_candidate.json`
